{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Predicting Links with Traditional Methods**\n",
        "- Link prediction is the problem of predicting the existence of a link between two nodes.\n",
        "#### **Heuristic Techniques**\n",
        "- Heuristic techniques can be divided into two categories - local (1-hop and 2-hop neighbours) and global.\n",
        "- Local heuristics measure the similarity between two nodes by considering their local neighbourhood.\n",
        "1. Common neighours counts the number of 1-hop neighbours two nodes have in common.\n",
        "$$f(u,v) = |N(u) \\cap N(v)|$$\n",
        "2. Jaccard's coefficient measures the proportion of 1-hop neighbours shared by two nodes. It normalizes the result by the total number of neighbours so that it rewards nodes with few interconnected neighbours instead of nodes with high degrees.\n",
        "$$f(u,v) = \\frac{|N(u) \\cap N(v)|}{|N(u) \\cup N(v)|}$$\n",
        "3. Adamic-Adar index sums the inverse logarithmic degree of neighbours shared by the two target nodes. The idea is that common neighbours with large neighbourhoods are less significant than those with small neighbourhoods.\n",
        "$$f(u,v) = \\sum_{x \\in N(u) \\cap N(v)} \\frac{1}{log |N(x)|}$$\n",
        "- Global heuristics consider an entire network instead of a local neighbourhood.\n",
        "1. Katz index computes the weighted sum of every possible path between two nodes. Weights correspond to a discount factor $\\beta \\in [0,1]$. Two nodes are more likely to be connected if there are many short paths between them. Note that paths of any length can be calculated using adjacency matrix powers $A^n$.\n",
        "$$f(u,v) = \\sum_{i=1}^{\\infty} \\beta^i A^i$$\n",
        "2. Random walk with restart performs random walks, starting from a target node. After each walk, it increases the visit count of the current node. With an $\\alpha$ probability, it restarts the walk at the target node. Otherwise, it continues its random walk. After a predefined number of iterations, we stop the algorithm and suggest links between target node and nodes with highest visit counts.\n",
        "\n",
        "#### **Matrix Factorization**\n",
        "- Matrix factorization predicts links by predicting the entire adjacency matrix $\\hat{A}$. This is performed using node embeddings. Similar nodes $u,v$ should have similar embeddings $z_u, z_v$.\n",
        "- If two nodes are similar, $z_uz_v^T$ should be maximal. If two nodes are different, $z_uz_v^T$ should be minimal.\n",
        "$$A_{uv} \\approx z_v^Tz_u$$\n",
        "- The goal is to minimize the L2-norm between $A_{uv}$ and $z_v^Tz_u$.\n",
        "$$min_z \\sum_{i \\in V, j \\in V} (A_{uv} - z_v^Tz_u)^2$$\n",
        "- However, matrix factorization cannot use node features and cannot capture structural similarity.\n",
        "\n",
        "## **Predicting Links with Node Embeddings**\n",
        "#### **Graph Autoencoders**\n",
        "- The encoder is a two layer GCN that computes node embeddings. Note that the encoder can be replaced with another type of GNN, for example GraphSAGE.\n",
        "$$Z = GCN(X, A)$$\n",
        "- The decoder approximates the adjacency matrix $\\hat{A}$ using matrix factorization and a sigmoid function $\\sigma$ to output probabilities for each element of the adjacency matrix.\n",
        "$$\\hat{A} = \\sigma(Z^TZ)$$\n",
        "- The GAE is trained using the binary cross-entropy loss between elements of both adjacency matrices.\n",
        "$$L_{BCE} = \\sum_{i \\in V, j \\in V} -A_{ij} log(\\hat{A_{ij}}) - (1-A_{ij})log(1 - \\hat{A_{ij}})$$\n",
        "\n",
        "#### **Graph Variational Autoencoders**\n",
        "- Instead of directly learning node embeddings, VGAE learns normal distributions that are sampled to produce embeddings.\n",
        "1. The encoder is composed of two GCNs that share their first layer. The objective is to learn the parameters of each latent normal distributions - a mean $\\mu_i$ (learned by $GCN_{mu}$) and a variance $\\sigma_i^2$ (learned by $GCN_{\\sigma}$).\n",
        "2. The decoder samples embeddings $z_i$ from the learned distributions $N(\\mu_i,\\sigma_i^2)$ and $\\hat{A} = \\sigma(Z^TZ)$.\n",
        "- Kullback-Leiber (KL) divergence is added to the loss function to ensure that the encoder's output follows a normal distribution. This is known as evidence lower bound (ELBO).\n",
        "$$L_{ELBO} = L_{BCE} - KL[q(Z|X,A) || p(Z)]$$\n",
        "where $q(Z|X,A)$ represents the encoder and $p(Z)$ is the prior distribution of $Z$.\n",
        "- The model's performance is generally evaluated using AU-ROC and average precision.\n",
        "\n",
        "## **Implementing VGAE**"
      ],
      "metadata": {
        "id": "9czrVJbBvA_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MOc4dyJ3dUo",
        "outputId": "53ba798f-2e6a-4f2a-8c85-10c4947a0b34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.8/994.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
        "])\n",
        "\n",
        "dataset = Planetoid('.', name='Cora', transform=transform)\n",
        "\n",
        "train_data, val_data, test_data = dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NrQIK124518",
        "outputId": "7f84000f-241b-46f0-87fd-072add3513c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/planetoid.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv, VGAE\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, dim_in, dim_out):\n",
        "    super().__init__()\n",
        "    self.conv1 = GCNConv(dim_in, 2 * dim_out)\n",
        "    self.conv_mu = GCNConv(2 * dim_out, dim_out) # Learns mean\n",
        "    self.conv_logstd = GCNConv(2 * dim_out, dim_out) # Learn log of standard deviation\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index).relu()\n",
        "    return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
      ],
      "metadata": {
        "id": "ykUpBRdA5Tv0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGAE(Encoder(dataset.num_features, 16)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "def train():\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  z = model.encode(train_data.x, train_data.edge_index)\n",
        "  loss = model.recon_loss(z, train_data.pos_edge_label_index) + (1 / train_data.num_nodes) * model.kl_loss()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return float(loss)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
        "\n",
        "for epoch in range(301):\n",
        "    loss = train()\n",
        "    val_auc, val_ap = test(test_data)\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}')\n",
        "\n",
        "test_auc, test_ap = test(test_data)\n",
        "print(f'Test AUC: {test_auc:.4f} | Test AP {test_ap:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrAueGo86FfL",
        "outputId": "0d2a5d30-2fee-460c-e3a6-4c8e7be6db87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  0 | Loss: 3.4533 | Val AUC: 0.6614 | Val AP: 0.6886\n",
            "Epoch 50 | Loss: 1.3308 | Val AUC: 0.6665 | Val AP: 0.6830\n",
            "Epoch 100 | Loss: 1.1945 | Val AUC: 0.7290 | Val AP: 0.7204\n",
            "Epoch 150 | Loss: 1.0716 | Val AUC: 0.7947 | Val AP: 0.7941\n",
            "Epoch 200 | Loss: 1.0033 | Val AUC: 0.8444 | Val AP: 0.8421\n",
            "Epoch 250 | Loss: 0.9740 | Val AUC: 0.8507 | Val AP: 0.8484\n",
            "Epoch 300 | Loss: 0.9479 | Val AUC: 0.8887 | Val AP: 0.8797\n",
            "Test AUC: 0.8887 | Test AP 0.8797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = model.encode(test_data.x, test_data.edge_index)\n",
        "Ahat = torch.sigmoid(z @ z.T)\n",
        "Ahat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQIHmKxG6un5",
        "outputId": "71fb110c-1b48-4787-be49-69ec464168b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8625, 0.7527, 0.8332,  ..., 0.3899, 0.8422, 0.7917],\n",
              "        [0.7527, 0.8153, 0.8529,  ..., 0.4861, 0.8352, 0.7916],\n",
              "        [0.8332, 0.8529, 0.8979,  ..., 0.4606, 0.8877, 0.8433],\n",
              "        ...,\n",
              "        [0.3899, 0.4861, 0.4606,  ..., 0.6122, 0.4758, 0.4470],\n",
              "        [0.8422, 0.8352, 0.8877,  ..., 0.4758, 0.8948, 0.8351],\n",
              "        [0.7917, 0.7916, 0.8433,  ..., 0.4470, 0.8351, 0.7928]],\n",
              "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gERi7uFb616g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}