{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Introducing GraphSAGE**\n",
        "- GraphSAGE is a GNN architecture designed to handle large graphs (with over 100000 nodes) and is adopted by tech companies such as UberEats and Pinterest. It solves two issues with GCN and GAT - scaling to large graphs and efficiently generalizing to unseen data.\n",
        "\n",
        "#### **Neighbour Sampling**\n",
        "- Since every GNN layer computes node embeddings based on their neighbours, computing an embedding only requires the direct neighbours of this node (1 hop) and if a GNN has two layers, we need these neighbours and their own neighbours (2 hops).\n",
        "- The 2-hop neighbours are aggregated to compute the embedding of 1-hop neighbours, which are aggreated to compute the embedding of a central node.\n",
        "- However, the computation graph become exponentially large with respect to the number of hops and nodes with high degrees of connectivity create enormous computation graphs. Hence, neighbour sampling is used to limit the size of computation graphs.\n",
        "\n",
        "#### **Aggregation**\n",
        "There are three methods to compute embeddings given selected neighbouring nodes.\n",
        "1. Mean aggregator\n",
        "- The mean aggregator takes the embeddings of target nodes and their sampled neighbours and average them. A linear transformation with a weight matrix $W$ is applied and a non-linear transformation using ReLU or tanh is finally applied.\n",
        "$$h_i' = \\sigma(W_1h_i + W_2 mean_{j \\in N_i} (h_j))$$\n",
        "2. LSTM aggregator\n",
        "3. Pooling aggregator\n",
        "- Every neighbour's embedding is fed to a multi-layer perceptron to produce a new vector. An elementwise max operation is performed to only keep the highest value for each feature."
      ],
      "metadata": {
        "id": "cZPalQNr1HjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementing GraphSAGE to classify nodes on PubMed**"
      ],
      "metadata": {
        "id": "9ayyofhE1oMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        "torch.manual_seed(-1)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "DvIrOWDz0mj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97e548a-c761-4950-a0f2-002a334db8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.8/994.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "dataset = Planetoid(root='.', name=\"Pubmed\")\n",
        "data = dataset[0]\n",
        "\n",
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {data.x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print(f'Training nodes: {sum(data.train_mask).item()}')\n",
        "print(f'Evaluation nodes: {sum(data.val_mask).item()}')\n",
        "print(f'Test nodes: {sum(data.test_mask).item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqdDSlq81GlL",
        "outputId": "13c6d3e6-bcda-4289-8f65-d32540d242a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/planetoid.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Pubmed()\n",
            "-------------------\n",
            "Number of graphs: 1\n",
            "Number of nodes: 19717\n",
            "Number of features: 500\n",
            "Number of classes: 3\n",
            "Training nodes: 60\n",
            "Evaluation nodes: 500\n",
            "Test nodes: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "# Create batches with neighbor sampling\n",
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[10, 10], # Keep 10 neighbours of target node and 10 of their own neighbours\n",
        "    batch_size=16, # Group 60 target nodes into batches of 16 nodes, which result in four batches\n",
        "    input_nodes=data.train_mask)"
      ],
      "metadata": {
        "id": "HSkt4Y_f1Slq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(1)\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    \"\"\"GraphSAGE\"\"\"\n",
        "    def __init__(self, dim_in, dim_h, dim_out):\n",
        "        super().__init__()\n",
        "        self.sage1 = SAGEConv(dim_in, dim_h)\n",
        "        self.sage2 = SAGEConv(dim_h, dim_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.sage1(x, edge_index)\n",
        "        h = torch.relu(h)\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.sage2(h, edge_index)\n",
        "        return h\n",
        "\n",
        "    def fit(self, loader, epochs):\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
        "\n",
        "        self.train()\n",
        "        for epoch in range(epochs+1):\n",
        "            total_loss = 0\n",
        "            acc = 0\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "\n",
        "            # Train on batches\n",
        "            for batch in loader:\n",
        "                optimizer.zero_grad()\n",
        "                out = self(batch.x, batch.edge_index)\n",
        "                loss = criterion(out[batch.train_mask], batch.y[batch.train_mask])\n",
        "                total_loss += loss.item()\n",
        "                acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Validation\n",
        "                val_loss += criterion(out[batch.val_mask], batch.y[batch.val_mask])\n",
        "                val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n",
        "\n",
        "            # Print metrics every 10 epochs\n",
        "            if epoch % 20 == 0:\n",
        "                print(f'Epoch {epoch:>3} | Train Loss: {loss/len(loader):.3f} | Train Acc: {acc/len(loader)*100:>6.2f}% | Val Loss: {val_loss/len(train_loader):.2f} | Val Acc: {val_acc/len(train_loader)*100:.2f}%')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(self, data):\n",
        "        self.eval()\n",
        "        out = self(data.x, data.edge_index)\n",
        "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
        "        return acc"
      ],
      "metadata": {
        "id": "m0trmavx1YJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sage = GraphSAGE(dataset.num_features, 64, dataset.num_classes)\n",
        "print(sage)\n",
        "\n",
        "sage.fit(train_loader, epochs = 200)\n",
        "acc = sage.test(data)\n",
        "print(f'\\nGraphSAGE test accuracy: {acc*100:.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITefVFhh1dMg",
        "outputId": "523bfdbd-4153-4c2c-ff27-8720a99efa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GraphSAGE(\n",
            "  (sage1): SAGEConv(500, 64, aggr=mean)\n",
            "  (sage2): SAGEConv(64, 3, aggr=mean)\n",
            ")\n",
            "Epoch   0 | Train Loss: 0.311 | Train Acc:  21.42% | Val Loss: 1.12 | Val Acc: 24.79%\n",
            "Epoch  20 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.78 | Val Acc: 63.39%\n",
            "Epoch  40 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.64 | Val Acc: 75.12%\n",
            "Epoch  60 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.75 | Val Acc: 77.50%\n",
            "Epoch  80 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.51 | Val Acc: 78.33%\n",
            "Epoch 100 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.72 | Val Acc: 77.11%\n",
            "Epoch 120 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.80 | Val Acc: 71.04%\n",
            "Epoch 140 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.76 | Val Acc: 70.00%\n",
            "Epoch 160 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.59 | Val Acc: 73.19%\n",
            "Epoch 180 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.53 | Val Acc: 86.81%\n",
            "Epoch 200 | Train Loss: 0.000 | Train Acc: 100.00% | Val Loss: 0.92 | Val Acc: 69.64%\n",
            "\n",
            "GraphSAGE test accuracy: 75.90%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}