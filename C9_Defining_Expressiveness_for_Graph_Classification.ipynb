{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining Expressiveness**\n",
        "- Neural networks are used to approximate functions, as justified by the universal approximation theorem, which states that a feedforward neural network with only one layer can approximate any smoooth function.\n",
        "- The goal of graph neural networks is to produce the best node embeddings possible. To distinguish nodes, we compare node features and their neighbours. This problem is called graph isomorphism problem in graph theory.\n",
        "- Two graphs are isomorphic if they have the same connections, and their only difference is a permutation of their nodes.\n",
        "- The Weisfeiler-Lehman test (WL-test) aims to build a caconical form of a graph, and compares the canonical form of two graphs to check whether they are isomorphic.\n",
        "1. At the beginning, each node in the graph receives the same colour.\n",
        "2. Each node aggregates its own colour and the colours of its neighbours.\n",
        "3. The result is fed to a hash function that produces a new colour.\n",
        "4. Each node aggregates its new colour and the new colour of its neighbours.\n",
        "5. The result is fed to a hash function that produces a new colour.\n",
        "6. The steps are repeated until no more node changes colour.\n",
        "- If two graphs do not share the same colours, they are not isomorphic. However, we cannot be sure if they are isomorphic if they share the same colours.\n",
        "- A sum aggregator can discriminate more graph structures than a mean or max aggregator. This implies that the aggregators used so far (for GCN, GAT, etc) are suboptimal since they are less expressive than a sum.\n",
        "\n",
        "## **Introducing GIN**\n",
        "- GIN is designed to be as expressive as the WL-test. GIN consists of two functions.\n",
        "1. Aggregate: The function $f$ selects the neighbouring nodes that the GNN considers.\n",
        "2. The function $\\phi$ combines the embeddings from the selected nodes to produce the new embeddings of the target nodes.\n",
        "\n",
        "$$h_i' = \\phi(h_i, f(\\{h_j: j \\in N_i\\}))$$\n",
        "\n",
        "- In the case of GCN, $f$ aggregates every neighbour of node $i$ and $\\phi$ applies a mean aggregator. In the case of GraphSAGE, $f$ is a neighbourhood sampling function and $\\phi$ can be a mean, max or LSTM aggregator.\n",
        "\n",
        "- The functions for GIN are designed to be injective. If the functions were not injective, same output would be produced for different inputs and embeddings would be less valuable since they contain less information.\n",
        "- Both functions can be learned using a single multi-layer perceptron, thanks to the universal approximation theorem. However, we should have more than one layer of MLP to distinguish specific graph structures.\n",
        "\n",
        "$$h_i' = MLP((1+\\epsilon)h_i + \\sum_{j \\in N_i} h_j)$$\n",
        "\n",
        "## **Classifying Graphs using GIN**\n",
        "- Graph classification is based on the node embeddings that a GNN produces. This operation is called global pooling. There are three ways to implement global pooling.\n",
        "1. Mean global pooling\n",
        "$$h_G = \\frac{1}{N} \\sum_{i=0}^{N}h_i$$\n",
        "2. Max global pooling\n",
        "$$h_G=max_{i=0}^N (h_i)$$\n",
        "3. Sum global pooling\n",
        "$$h_g = \\sum_{i=0}^N h_i$$\n",
        "\n",
        "- However, to consider all structural information, we need to consider embeddings produced by every layer of the GNN. Hence, we concatenate the sum of node embeddings by each layer of the GNN.\n",
        "\n",
        "$$h_G = \\sum_{i=0}^N h_i^0 || \\cdots || \\sum_{i=0}^N h_i^k$$\n",
        "\n",
        "## **Implementing GIN**\n",
        "- This dataset comprises 1,113 graphs representing proteins, where every node is an amino acid. An edge connects two nodes when their distance is lower than 0.6 nanometers. The goal of this dataset is to classify each protein as an enzyme."
      ],
      "metadata": {
        "id": "Wdq4kFkhmHVg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtDkr0wtmBbw",
        "outputId": "83230529-37ab-4add-a597-d534addcd3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.8/994.8 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        "torch.manual_seed(11)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='.', name='PROTEINS').shuffle()\n",
        "\n",
        "# Print information about the dataset\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-----------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {dataset[0].x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olZ8TiMcrFvB",
        "outputId": "17f0e6f3-64f7-4d67-a7c7-2412654d2113"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/PROTEINS.zip\n",
            "Extracting ./PROTEINS/PROTEINS.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: PROTEINS(1113)\n",
            "-----------------------\n",
            "Number of graphs: 1113\n",
            "Number of nodes: 14\n",
            "Number of features: 3\n",
            "Number of classes: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/tu_dataset.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  out = torch.load(self.processed_paths[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# Create training, validation, and test sets\n",
        "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
        "val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
        "test_dataset  = dataset[int(len(dataset)*0.9):]\n",
        "\n",
        "print(f'Training set   = {len(train_dataset)} graphs')\n",
        "print(f'Validation set = {len(val_dataset)} graphs')\n",
        "print(f'Test set       = {len(test_dataset)} graphs')\n",
        "\n",
        "# Create mini-batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "print('\\nTrain loader:')\n",
        "for i, batch in enumerate(train_loader):\n",
        "    print(f' - Batch {i}: {batch}')\n",
        "\n",
        "print('\\nValidation loader:')\n",
        "for i, batch in enumerate(val_loader):\n",
        "    print(f' - Batch {i}: {batch}')\n",
        "\n",
        "print('\\nTest loader:')\n",
        "for i, batch in enumerate(test_loader):\n",
        "    print(f' - Batch {i}: {batch}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vy3DHXErYzJ",
        "outputId": "6037b303-f907-465c-e593-9ffed744f614"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set   = 890 graphs\n",
            "Validation set = 111 graphs\n",
            "Test set       = 112 graphs\n",
            "\n",
            "Train loader:\n",
            " - Batch 0: DataBatch(edge_index=[2, 9274], x=[2468, 3], y=[64], batch=[2468], ptr=[65])\n",
            " - Batch 1: DataBatch(edge_index=[2, 8972], x=[2366, 3], y=[64], batch=[2366], ptr=[65])\n",
            " - Batch 2: DataBatch(edge_index=[2, 8820], x=[2350, 3], y=[64], batch=[2350], ptr=[65])\n",
            " - Batch 3: DataBatch(edge_index=[2, 9596], x=[2570, 3], y=[64], batch=[2570], ptr=[65])\n",
            " - Batch 4: DataBatch(edge_index=[2, 9108], x=[2490, 3], y=[64], batch=[2490], ptr=[65])\n",
            " - Batch 5: DataBatch(edge_index=[2, 10022], x=[2637, 3], y=[64], batch=[2637], ptr=[65])\n",
            " - Batch 6: DataBatch(edge_index=[2, 9732], x=[2726, 3], y=[64], batch=[2726], ptr=[65])\n",
            " - Batch 7: DataBatch(edge_index=[2, 9316], x=[2533, 3], y=[64], batch=[2533], ptr=[65])\n",
            " - Batch 8: DataBatch(edge_index=[2, 7994], x=[2074, 3], y=[64], batch=[2074], ptr=[65])\n",
            " - Batch 9: DataBatch(edge_index=[2, 11984], x=[3267, 3], y=[64], batch=[3267], ptr=[65])\n",
            " - Batch 10: DataBatch(edge_index=[2, 8544], x=[2241, 3], y=[64], batch=[2241], ptr=[65])\n",
            " - Batch 11: DataBatch(edge_index=[2, 9000], x=[2364, 3], y=[64], batch=[2364], ptr=[65])\n",
            " - Batch 12: DataBatch(edge_index=[2, 9122], x=[2443, 3], y=[64], batch=[2443], ptr=[65])\n",
            " - Batch 13: DataBatch(edge_index=[2, 8270], x=[2241, 3], y=[58], batch=[2241], ptr=[59])\n",
            "\n",
            "Validation loader:\n",
            " - Batch 0: DataBatch(edge_index=[2, 7738], x=[2021, 3], y=[64], batch=[2021], ptr=[65])\n",
            " - Batch 1: DataBatch(edge_index=[2, 6406], x=[1726, 3], y=[47], batch=[1726], ptr=[48])\n",
            "\n",
            "Test loader:\n",
            " - Batch 0: DataBatch(edge_index=[2, 9600], x=[2670, 3], y=[64], batch=[2670], ptr=[65])\n",
            " - Batch 1: DataBatch(edge_index=[2, 8590], x=[2284, 3], y=[48], batch=[2284], ptr=[49])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool"
      ],
      "metadata": {
        "id": "7uPKPfXirg2H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the composition of the GIN layer, we need an MLP with at least two layers. We should introduce batch normaliation to standardise the inputs of each hidden layer, which stabilizes and speeds up training. In summary, our GIN layer has the following composition:\n",
        "\n",
        "$$ Linear \\to BatchNorm \\to ReLU \\to Linear \\to ReLU$$"
      ],
      "metadata": {
        "id": "GS_Cy3_cshLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"GIN\"\"\"\n",
        "    def __init__(self, dim_h):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(dataset.num_node_features, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
        "        self.lin2 = Linear(dim_h*3, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node embeddings\n",
        "        h1 = self.conv1(x, edge_index)\n",
        "        h2 = self.conv2(h1, edge_index)\n",
        "        h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "        # Graph-level readout\n",
        "        h1 = global_add_pool(h1, batch)\n",
        "        h2 = global_add_pool(h2, batch)\n",
        "        h3 = global_add_pool(h3, batch)\n",
        "\n",
        "        # Concatenate graph embeddings\n",
        "        h = torch.cat((h1, h2, h3), dim=1)\n",
        "\n",
        "        # Classifier\n",
        "        h = self.lin1(h)\n",
        "        h = h.relu()\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h = self.lin2(h)\n",
        "\n",
        "        return F.log_softmax(h, dim=1)"
      ],
      "metadata": {
        "id": "nkx4b32Hsedd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    epochs = 100\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs+1):\n",
        "        total_loss = 0\n",
        "        acc = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        # Train on batches\n",
        "        for data in loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss / len(loader)\n",
        "            acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc = test(model, val_loader)\n",
        "\n",
        "        # Print metrics every 20 epochs\n",
        "        if(epoch % 20 == 0):\n",
        "            print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss += criterion(out, data.y) / len(loader)\n",
        "        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()\n",
        "\n",
        "gin = GIN(dim_h=32)\n",
        "gin = train(gin, train_loader)\n",
        "test_loss, test_acc = test(gin, test_loader)\n",
        "print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofL5GpSarrWE",
        "outputId": "a35fff38-2aa4-49da-90da-093570815709"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 1.07 | Train Acc: 61.45% | Val Loss: 0.60 | Val Acc: 61.32%\n",
            "Epoch  20 | Train Loss: 0.55 | Train Acc: 75.30% | Val Loss: 0.52 | Val Acc: 76.86%\n",
            "Epoch  40 | Train Loss: 0.50 | Train Acc: 74.93% | Val Loss: 0.52 | Val Acc: 75.23%\n",
            "Epoch  60 | Train Loss: 0.50 | Train Acc: 75.89% | Val Loss: 0.53 | Val Acc: 77.58%\n",
            "Epoch  80 | Train Loss: 0.48 | Train Acc: 76.37% | Val Loss: 0.47 | Val Acc: 80.83%\n",
            "Epoch 100 | Train Loss: 0.47 | Train Acc: 78.78% | Val Loss: 0.49 | Val Acc: 74.73%\n",
            "Test Loss: 0.55 | Test Acc: 72.14%\n"
          ]
        }
      ]
    }
  ]
}